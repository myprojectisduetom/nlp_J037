{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "2cc13d01",
        "outputId": "fa67f756-4248-475d-ff74-c410954d4d3a"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.0\n",
            "    Uninstalling scipy-1.16.0:\n",
            "      Successfully uninstalled scipy-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "1a9e495c99954ad7995a7fa81370bdc5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhX6XG7iJnvf",
        "outputId": "e869e4fa-f878-4253-ace2-6979fc48a0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pretrained Word2Vec Google News model\n",
        "wv_pretrained = api.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Task 1: Find similar words ---\n",
        "words = ['king', 'apple', 'computer', 'music', 'university']\n",
        "\n",
        "for word in words:\n",
        "    print(f\"\\nTop 5 words similar to '{word}':\")\n",
        "    for sim_word, score in wv_pretrained.most_similar(word, topn=5):\n",
        "        print(f\"{sim_word}: {score:.4f}\")\n",
        "\n",
        "# --- Task 2: Word Vector Arithmetic (Analogies) ---\n",
        "print(\"\\n=== Vector Arithmetic Analogies ===\")\n",
        "analogies = [\n",
        "    (\"king\", \"man\", \"woman\"),     # Expected: queen\n",
        "    (\"Paris\", \"France\", \"Germany\"),  # Expected: Berlin\n",
        "    (\"walking\", \"walk\", \"swim\")   # Expected: swimming\n",
        "]\n",
        "\n",
        "for a, b, c in analogies:\n",
        "    result = wv_pretrained.most_similar(positive=[c, a], negative=[b], topn=1)\n",
        "    print(f\"{a} - {b} + {c} ≈ {result[0][0]} (Score: {result[0][1]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYLNzhPeKadd",
        "outputId": "6e294e75-50ca-4c97-a849-dc2964a712d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 words similar to 'king':\n",
            "kings: 0.7138\n",
            "queen: 0.6511\n",
            "monarch: 0.6413\n",
            "crown_prince: 0.6204\n",
            "prince: 0.6160\n",
            "\n",
            "Top 5 words similar to 'apple':\n",
            "apples: 0.7204\n",
            "pear: 0.6451\n",
            "fruit: 0.6410\n",
            "berry: 0.6302\n",
            "pears: 0.6134\n",
            "\n",
            "Top 5 words similar to 'computer':\n",
            "computers: 0.7979\n",
            "laptop: 0.6640\n",
            "laptop_computer: 0.6549\n",
            "Computer: 0.6473\n",
            "com_puter: 0.6082\n",
            "\n",
            "Top 5 words similar to 'music':\n",
            "classical_music: 0.7198\n",
            "jazz: 0.6835\n",
            "Music: 0.6596\n",
            "Without_Donny_Kirshner: 0.6416\n",
            "songs: 0.6396\n",
            "\n",
            "Top 5 words similar to 'university':\n",
            "universities: 0.7004\n",
            "faculty: 0.6781\n",
            "unversity: 0.6758\n",
            "undergraduate: 0.6587\n",
            "univeristy: 0.6585\n",
            "\n",
            "=== Vector Arithmetic Analogies ===\n",
            "king - man + woman ≈ queen (Score: 0.7118)\n",
            "Paris - France + Germany ≈ Berlin (Score: 0.7644)\n",
            "walking - walk + swim ≈ swimming (Score: 0.8246)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn nltk gensim tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNrQZh9cOyj6",
        "outputId": "157b54d7-4f7a-44e9-d095-dd8ead27c909"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim.downloader import load\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQTlbX_IO5ZW",
        "outputId": "d2246457-5ce4-46d1-f5d9-465b5c711b7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import gensim\n",
        "# --- Load dataset safely using Python's csv module ---\n",
        "data = []\n",
        "with open('IMDB Dataset.csv', 'r', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # Skip header row\n",
        "    for row in reader:\n",
        "        if len(row) == 2:\n",
        "            data.append({'review': row[0], 'sentiment': row[1]})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "\n",
        "# --- Clean + tokenize ---\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    tokens = text.lower().split()\n",
        "    return [w for w in tokens if w not in stop_words]\n",
        "\n",
        "df['tokens'] = df['review'].apply(preprocess)\n",
        "\n",
        "# --- Train/Test split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tokens'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Vector Averaging ---\n",
        "def get_vectors(tokens_list, model, vector_size):\n",
        "    vectors = []\n",
        "    # Check if the model is a KeyedVectors object (from gensim.downloader)\n",
        "    if isinstance(model, gensim.models.keyedvectors.KeyedVectors):\n",
        "        for tokens in tqdm(tokens_list):\n",
        "            word_vecs = [model[word] for word in tokens if word in model]\n",
        "            if word_vecs:\n",
        "                vectors.append(np.mean(word_vecs, axis=0))\n",
        "            else:\n",
        "                vectors.append(np.zeros(vector_size))\n",
        "    else: # For custom-trained Word2Vec/FastText models\n",
        "        for tokens in tqdm(tokens_list):\n",
        "            word_vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
        "            if word_vecs:\n",
        "                vectors.append(np.mean(word_vecs, axis=0))\n",
        "            else:\n",
        "                vectors.append(np.zeros(vector_size))\n",
        "    return np.array(vectors)\n",
        "\n",
        "# --- 1. Pretrained Word2Vec ---\n",
        "print(\"Using Pretrained Word2Vec...\")\n",
        "pre_w2v = load(\"word2vec-google-news-300\")\n",
        "X_train_wv = get_vectors(X_train, pre_w2v, 300)\n",
        "X_test_wv = get_vectors(X_test, pre_w2v, 300)\n",
        "clf_wv = LogisticRegression(max_iter=1000)\n",
        "clf_wv.fit(X_train_wv, y_train)\n",
        "acc_wv = accuracy_score(y_test, clf_wv.predict(X_test_wv))\n",
        "\n",
        "# --- 2. Custom Skip-gram Word2Vec ---\n",
        "print(\"Training custom Skip-gram Word2Vec...\")\n",
        "custom_sg = Word2Vec(sentences=X_train.tolist(), sg=1, vector_size=100, window=3, min_count=1, workers=4).wv\n",
        "X_train_sg = get_vectors(X_train, custom_sg, 100)\n",
        "X_test_sg = get_vectors(X_test, custom_sg, 100)\n",
        "clf_sg = LogisticRegression(max_iter=1000)\n",
        "clf_sg.fit(X_train_sg, y_train)\n",
        "acc_sg = accuracy_score(y_test, clf_sg.predict(X_test_sg))\n",
        "\n",
        "# --- 3. Custom CBOW Word2Vec ---\n",
        "print(\"Training custom CBOW Word2Vec...\")\n",
        "custom_cb = Word2Vec(sentences=X_train.tolist(), sg=0, vector_size=100, window=3, min_count=1, workers=4).wv\n",
        "X_train_cb = get_vectors(X_train, custom_cb, 100)\n",
        "X_test_cb = get_vectors(X_test, custom_cb, 100)\n",
        "clf_cb = LogisticRegression(max_iter=1000)\n",
        "clf_cb.fit(X_train_cb, y_train)\n",
        "acc_cb = accuracy_score(y_test, clf_cb.predict(X_test_cb))\n",
        "\n",
        "# --- 4. Custom FastText ---\n",
        "print(\"Training custom FastText...\")\n",
        "custom_ft = FastText(sentences=X_train.tolist(), sg=1, vector_size=100, window=3, min_count=1, workers=4).wv\n",
        "X_train_ft = get_vectors(X_train, custom_ft, 100)\n",
        "X_test_ft = get_vectors(X_test, custom_ft, 100)\n",
        "clf_ft = LogisticRegression(max_iter=1000)\n",
        "clf_ft.fit(X_train_ft, y_train)\n",
        "acc_ft = accuracy_score(y_test, clf_ft.predict(X_test_ft))\n",
        "\n",
        "# --- Results ---\n",
        "print(\"\\n=== Model Accuracy Summary ===\")\n",
        "print(f\"Pretrained Word2Vec  : {acc_wv:.4f}\")\n",
        "print(f\"Custom Skip-gram W2V : {acc_sg:.4f}\")\n",
        "print(f\"Custom CBOW W2V      : {acc_cb:.4f}\")\n",
        "print(f\"Custom FastText      : {acc_ft:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLtFf6jrO-R0",
        "outputId": "dd1d5b41-dcc4-472c-e80c-4f9d3c66b6e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Pretrained Word2Vec...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5670/5670 [00:01<00:00, 3878.82it/s]\n",
            "100%|██████████| 1418/1418 [00:00<00:00, 3845.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training custom Skip-gram Word2Vec...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5670/5670 [00:01<00:00, 4750.97it/s]\n",
            "100%|██████████| 1418/1418 [00:00<00:00, 5018.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training custom CBOW Word2Vec...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5670/5670 [00:01<00:00, 5131.42it/s]\n",
            "100%|██████████| 1418/1418 [00:00<00:00, 4768.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training custom FastText...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5670/5670 [00:01<00:00, 4920.34it/s]\n",
            "100%|██████████| 1418/1418 [00:00<00:00, 3065.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Model Accuracy Summary ===\n",
            "Pretrained Word2Vec  : 0.8406\n",
            "Custom Skip-gram W2V : 0.8096\n",
            "Custom CBOW W2V      : 0.7320\n",
            "Custom FastText      : 0.7898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rk_D6ykWA4HZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}